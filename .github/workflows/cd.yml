name: Databricks AWS Setup
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
jobs:
  create-aws-resource:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      TF_VAR_databricks_account_username: ${{ secrets.DATABRICKS_ACCOUNT_USERNAME }}
      TF_VAR_databricks_account_password: ${{ secrets.DATABRICKS_ACCOUNT_PASSWORD }}
      TF_VAR_databricks_account_id: ${{ secrets.DATABRICKS_ACCOUNT_ID }}
      TF_VAR_email: ${{secrets.AWS_SNS_EMAIL}}
      TF_VAR_AWS_S3_TF_BUCKET: ${{secrets.AWS_S3_TF_BUCKET}}

    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - uses: actions/setup-python@v4

      - name: AWS S3 Copy Source Data
        uses: jakejarvis/s3-sync-action@v0.5.0
        with:
          args: --follow-symlinks
        env:
          AWS_S3_BUCKET: "data-engg-onboarding"

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16.x

      - name: Setup Serverless Framework & Build
        run: |
          npm install -g serverless
          cd dbt_gx
          serverless package

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1
        with:
          terraform_version: 1.5.6

      - name: Terraform Init (AWS Resource)
        run: |
          cd dbt_gx/aws_resources
          terraform init

      - name: Plan and Apply AWS resources
        run: |
          cd dbt_gx/aws_resources
          terraform apply -auto-approve
        id: aws_outputs

      - name: Terraform Init (Databricks)
        run: |
          cd dbt_gx/dbt_workspace
          terraform init

      - name: Plan and Apply Databricks workspace
        run: |
          cd dbt_gx/dbt_workspace
          terraform apply -auto-approve
        id: databricks_outputs

      - name: Store Terraform Outputs in S3
        run: |
          echo "S3 Bucket name ${{ steps.s3_bucket_name.outputs.s3_bucket_name }}" 
          S3_BUCKET_NAME=${{ steps.s3_bucket_name.outputs.s3_bucket_name }}
          # aws s3 cp dbt_gx/aws_resources/terraform.tfstate s3://${{secrets.AWS_S3_TF_BUCKET}}/tf-state
          # aws s3 cp dbt_gx/dbt_workspace/terraform.tfstate s3://${{secrets.AWS_S3_TF_BUCKET}}/tf-state

      - name: Destroy AWS Resources
        run: |
          cd dbt_gx/aws_resources
          terraform destroy

      - name: Destroy Databricks Resources
        run: |
          cd dbt_gx/dbt_workspace
          terraform destroy

      - name: Use Terraform Outputs
        run: |
          echo "SNS Topic ARN: ${{ steps.aws_outputs.outputs.sns_topic_arn }}"
          echo "S3 Bucket Name: ${{ steps.databricks_outputs.outputs.databricks_host }}"
          echo "Lambda Function ARN: ${{ steps.databricks_outputs.outputs.databricks_token }}"

          echo "::set-env name=SNS_TOPIC_ARN::${{ steps.aws_outputs.outputs.sns_topic_arn }}"
          echo "::set-env name=DATABRICKS_HOST::${{ steps.databricks_outputs.outputs.databricks_host }}"
          echo "::set-env name=DATABRICKS_TOKEN::${{ steps.databricks_outputs.outputs.databricks_token }}"

      - name: Update Lambda Environment Variables
        run: aws lambda update-function-configuration --function-name ${{ secrets.AWS_LAMBDA_NAME }} --environment "Variables={ARN_VALUE='SOME_RANDOM_VALUE'}"

      - name: Run Cluster Script
        run: |
          cd dbt_gx
          pip install databricks_api
          python3 cluster.py
